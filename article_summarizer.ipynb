{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.tokenize import sent_tokenize as sentTokenize\n",
    "import re\n",
    "import string\n",
    "\n",
    "path = \".articles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['косачи.txt',\n",
       " 'федерер.txt',\n",
       " 'подИгото1Гост.txt',\n",
       " 'chess.txt',\n",
       " 'littleredriddinghood.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "articles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = ['а',\n",
    "'добро',\n",
    "'ме',\n",
    "'първата',\n",
    "'автентичен',\n",
    "'добър',\n",
    "'между',\n",
    "'първи',\n",
    "'аз',\n",
    "'докато',\n",
    "'мек',\n",
    "'първо',\n",
    "'ако',\n",
    "'докога',\n",
    "'мен',\n",
    "'пъти',\n",
    "'ала',\n",
    "'дори',\n",
    "'месец',\n",
    "'равен',\n",
    "'бе',\n",
    "'досега',\n",
    "'ми',\n",
    "'равна',\n",
    "'без',\n",
    "'доста',\n",
    "'много',\n",
    "'с',\n",
    "'беше',\n",
    "'друг',\n",
    "'мнозина',\n",
    "'са',\n",
    "'би',\n",
    "'друга',\n",
    "'мога',\n",
    "'сам',\n",
    "'бивш',\n",
    "'други',\n",
    "'могат',\n",
    "'само',\n",
    "'бивша',\n",
    "'е',\n",
    "'може',\n",
    "'се',\n",
    "'бившо',\n",
    "'евтин',\n",
    "'мокър',\n",
    "'сега',\n",
    "'бил',\n",
    "'едва',\n",
    "'моля',\n",
    "'си',\n",
    "'била',\n",
    "'един',\n",
    "'момента',\n",
    "'син',\n",
    "'били',\n",
    "'една',\n",
    "'му',\n",
    "'скоро',\n",
    "'било',\n",
    "'еднаква',\n",
    "'н',\n",
    "'след',\n",
    "'благодаря',\n",
    "'еднакви',\n",
    "'на',\n",
    "'следващ',\n",
    "'близо',\n",
    "'еднакъв',\n",
    "'над',\n",
    "'сме',\n",
    "'бъдат',\n",
    "'едно',\n",
    "'назад',\n",
    "'смях',\n",
    "'бъде',\n",
    "'екип',\n",
    "'най',\n",
    "'според',\n",
    "'бяха',\n",
    "'ето',\n",
    "'направи',\n",
    "'сред',\n",
    "'в',\n",
    "'живот',\n",
    "'напред',\n",
    "'срещу',\n",
    "'вас',\n",
    "'за',\n",
    "'например',\n",
    "'сте',\n",
    "'ваш',\n",
    "'забавям',\n",
    "'нас',\n",
    "'съм',\n",
    "'ваша',\n",
    "'зад',\n",
    "'не',\n",
    "'със',\n",
    "'вероятно',\n",
    "'заедно',\n",
    "'него',\n",
    "'също',\n",
    "'вече',\n",
    "'заради',\n",
    "'нещо',\n",
    "'т',\n",
    "'взема',\n",
    "'засега',\n",
    "'нея',\n",
    "'тази',\n",
    "'ви',\n",
    "'заспал',\n",
    "'ни',\n",
    "'така',\n",
    "'вие',\n",
    "'затова',\n",
    "'ние',\n",
    "'такива',\n",
    "'винаги',\n",
    "'защо',\n",
    "'никой',\n",
    "'такъв',\n",
    "'внимава',\n",
    "'защото',\n",
    "'нито',\n",
    "'там',\n",
    "'време',\n",
    "'и',\n",
    "'нищо',\n",
    "'твой',\n",
    "'все',\n",
    "'из',\n",
    "'но',\n",
    "'те',\n",
    "'всеки',\n",
    "'или',\n",
    "'нов',\n",
    "'тези',\n",
    "'всички',\n",
    "'им',\n",
    "'нова',\n",
    "'ти',\n",
    "'всичко',\n",
    "'има',\n",
    "'нови',\n",
    "'всяка',\n",
    "'имат',\n",
    "'новина',\n",
    "'то',\n",
    "'във',\n",
    "'иска',\n",
    "'някои',\n",
    "'това',\n",
    "'въпреки',\n",
    "'й',\n",
    "'някой',\n",
    "'тогава',\n",
    "'върху',\n",
    "'каза',\n",
    "'няколко',\n",
    "'този',\n",
    "'г',\n",
    "'как',\n",
    "'няма',\n",
    "'той',\n",
    "'ги',\n",
    "'каква',\n",
    "'обаче',\n",
    "'толкова',\n",
    "'главен',\n",
    "'какво',\n",
    "'около',\n",
    "'точно',\n",
    "'главна',\n",
    "'както',\n",
    "'освен',\n",
    "'три',\n",
    "'главно',\n",
    "'какъв',\n",
    "'особено',\n",
    "'трябва',\n",
    "'глас',\n",
    "'като',\n",
    "'от',\n",
    "'тук',\n",
    "'го',\n",
    "'кога',\n",
    "'отгоре',\n",
    "'тъй',\n",
    "'година',\n",
    "'когато',\n",
    "'отново',\n",
    "'тя',\n",
    "'години',\n",
    "'което',\n",
    "'още',\n",
    "'тях',\n",
    "'годишен',\n",
    "'които',\n",
    "'пак',\n",
    "'у',\n",
    "'д',\n",
    "'кой',\n",
    "'по',\n",
    "'утре',\n",
    "'да',\n",
    "'който',\n",
    "'повече',\n",
    "'харесва',\n",
    "'дали',\n",
    "'колко',\n",
    "'повечето',\n",
    "'хиляди',\n",
    "'два',\n",
    "'която',\n",
    "'под',\n",
    "'ч',\n",
    "'двама',\n",
    "'къде',\n",
    "'поне',\n",
    "'часа',\n",
    "'двамата',\n",
    "'където',\n",
    "'поради',\n",
    "'че',\n",
    "'две',\n",
    "'към',\n",
    "'после',\n",
    "'често',\n",
    "'двете',\n",
    "'лесен',\n",
    "'почти',\n",
    "'чрез',\n",
    "'ден',\n",
    "'лесно',\n",
    "'прави',\n",
    "'ще',\n",
    "'днес',\n",
    "'ли',\n",
    "'пред',\n",
    "'щом',\n",
    "'дни',\n",
    "'лош',\n",
    "'преди',\n",
    "'юмрук',\n",
    "'до',\n",
    "'м',\n",
    "'през',\n",
    "'я',\n",
    "'добра',\n",
    "'май',\n",
    "'при',\n",
    "'як',\n",
    "'добре',\n",
    "'малко',\n",
    "'пък']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bulstem in /home/iarabadzhiyski/.local/lib/python3.8/site-packages (0.3.3)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install bulstem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 втор\n",
      "2 втори\n",
      "3 вторият\n",
      "прасков\n",
      "прасков\n",
      "прасков\n"
     ]
    }
   ],
   "source": [
    "from bulstem.stem import BulStemmer\n",
    "\n",
    "# just an example from https://github.com/mhardalov/bulstem-py\n",
    "# Pre-defined names of rule sets\n",
    "PRE_DEFINED_RULES = ['stem-context-1', \n",
    "                     'stem-context-2',\n",
    "                     'stem-context-3']\n",
    "                     \n",
    "# Excepted output:\n",
    "# 1 втор\n",
    "# 2 втори\n",
    "# 3 вторият\n",
    "for i, rules_name in enumerate(PRE_DEFINED_RULES, start=1):\n",
    "    stemmer = BulStemmer.from_file(rules_name, min_freq=2, left_context=i)\n",
    "    print(i, stemmer.stem('вторият'))\n",
    "\n",
    "stemmer = BulStemmer.from_file('stem-context-2', min_freq=2, left_context=1)\n",
    "def stem(word:str):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "\n",
    "print(stem(\"прасковата\"))\n",
    "print(stem(\"прасковите\"))\n",
    "print(stem(\"праскова\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitSentences(text:str):\n",
    "    return sentTokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyFilters(sentence, filters):\n",
    "    for f in filters:\n",
    "        sentence = f(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Gensim\n",
    "RE_PUNCT = re.compile('([%s])+' % re.escape(string.punctuation), re.UNICODE)\n",
    "def stripPunctuation(s):\n",
    "    return RE_PUNCT.sub(\" \", s)\n",
    "\n",
    "# Taken from Gensim\n",
    "RE_NUMERIC = re.compile(r\"[0-9]+\", re.UNICODE)\n",
    "def stripNumeric(s):\n",
    "    return RE_NUMERIC.sub(\"\", s)\n",
    "\n",
    "\n",
    "def removeStopwords(sentence):\n",
    "    return \" \".join(w for w in sentence.split() if w not in stopWords and len(w) != 1)\n",
    "\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    wordStems = [stem(word) for word in sentence.split()]\n",
    "    return \" \".join(wordStems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterWords(sentences):\n",
    "    filters = [lambda x: x.lower(), stripNumeric, stripPunctuation, removeStopwords,\n",
    "               stemSentence]\n",
    "    applyFiltersToToken = lambda token: applyFilters(token, filters)\n",
    "    return list(map(applyFiltersToToken, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntacticUnit(object):\n",
    "\n",
    "    def __init__(self, text, token=None, tag=None):\n",
    "        self.text = text\n",
    "        self.token = token\n",
    "        self.index = -1\n",
    "        self.score = -1\n",
    "        self.tag = tag\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Original: '\" + self.text + \"' *-*-*-* \" + \"Processed: '\" + self.token + \"'\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeSyntacticUnits(originalUnits, filteredUnits, tags=None):\n",
    "    units = []\n",
    "    for i in range(len(originalUnits)):\n",
    "        if filteredUnits[i] == '':\n",
    "            continue\n",
    "\n",
    "        text = originalUnits[i]\n",
    "        token = filteredUnits[i]\n",
    "        tag = tags[i][1] if tags else None\n",
    "        sentence = SyntacticUnit(text, token, tag)\n",
    "        sentence.index = i\n",
    "\n",
    "        units.append(sentence)\n",
    "\n",
    "    return units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCleanedSentences(path:str):\n",
    "    with open(path, encoding='utf-8') as article:\n",
    "        text =  ' '.join(article.read().splitlines())\n",
    "        originalSentences = splitSentences(text)\n",
    "        filteredSentences = filterWords(originalSentences)\n",
    "\n",
    "        return mergeSyntacticUnits(originalSentences, filteredSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    def __init__(self):\n",
    "        self.edges = dict()       # Mapping: Edge -> weight\n",
    "        self.adjList = dict()     # Pairing: Node -> Neighbors\n",
    "\n",
    "    def hasEdge(self, edge):\n",
    "        u,v = edge\n",
    "        return (u,v) in self.edges and (v,u) in self.edges\n",
    "\n",
    "    def edgeWeight(self, edge):\n",
    "        return self.edges[edge]\n",
    "\n",
    "    def neighbors(self, node):\n",
    "        return self.adjList[node]\n",
    "\n",
    "    def hasNode(self, node):\n",
    "        return node in self.adjList\n",
    "\n",
    "    def addEdge(self, edge, wt=1):\n",
    "        u, v = edge\n",
    "        if (v not in self.adjList[u] and u not in self.adjList[v]):\n",
    "            self.adjList[u].append(v)\n",
    "            self.edges[(u, v)] = wt\n",
    "            if (u != v):\n",
    "                self.adjList[v].append(u)\n",
    "                self.edges[(v, u)] = wt\n",
    "\n",
    "    def addNode(self, node):\n",
    "        if (not node in self.adjList):\n",
    "            self.adjList[node] = []\n",
    "        else:\n",
    "            raise ValueError(\"Node %s already in graph\" % node)\n",
    "\n",
    "    def nodes(self):\n",
    "        return list(self.adjList.keys())\n",
    "\n",
    "    def getEdges(self):\n",
    "        return [ a for a in list(self.edges.keys()) ]\n",
    "\n",
    "    def delNode(self, node):\n",
    "        for each in list(self.neighbors(node)):\n",
    "            if (each != node):\n",
    "                self.delEdge((each, node))\n",
    "        del(self.adjList[node])\n",
    "\n",
    "    def delEdge(self, edge):\n",
    "        u, v = edge\n",
    "        self.adjList[u].remove(v)\n",
    "        if (u != v):\n",
    "            self.adjList[v].remove(u)\n",
    "\n",
    "    def print(self):\n",
    "        for node in self.nodes():\n",
    "            print(f\"{node}: \", end='')\n",
    "            for adj in self.adjList[node]:\n",
    "                print(f\"{adj} ({self.edgeWeight((node , adj))}), \", end='')\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGraph(sentences):\n",
    "    graph = Graph()\n",
    "    for item in sentences:\n",
    "        if not graph.hasNode(item):\n",
    "            graph.addNode(item)\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGraph(graph):\n",
    "    nodes = graph.nodes()\n",
    "\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(len(nodes)):\n",
    "            if i == j:\n",
    "                continue\n",
    "\n",
    "            edge = (nodes[i], nodes[j])\n",
    "\n",
    "            if graph.hasEdge(edge):\n",
    "                graph.delEdge(edge)\n",
    "\n",
    "            graph.addEdge(edge, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countCommonWords(wordsSentence1, wordsSentence2):\n",
    "    return len(set(wordsSentence1) & set(wordsSentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10\n",
    "\n",
    "def getSimilarity(s1, s2):\n",
    "    wordsSentence1 = s1.split()\n",
    "    wordsSentence2 = s2.split()\n",
    "\n",
    "    commonWordCount = countCommonWords(wordsSentence1, wordsSentence2)\n",
    "\n",
    "    logS1 = log10(len(wordsSentence1))\n",
    "    logS2 = log10(len(wordsSentence2))\n",
    "\n",
    "    if logS1 + logS2 == 0:\n",
    "        return 0\n",
    "\n",
    "    return commonWordCount / (logS1 + logS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setGraphEdgeWeights(graph):\n",
    "    for sentence1 in graph.nodes():\n",
    "        for sentence2 in graph.nodes():\n",
    "\n",
    "            edge = (sentence1, sentence2)\n",
    "            if sentence1 != sentence2 and not graph.hasEdge(edge):\n",
    "                similarity = getSimilarity(sentence1, sentence2)\n",
    "                if similarity != 0:\n",
    "                    graph.addEdge(edge, similarity)\n",
    "\n",
    "    # Handles the case in which all similarities are zero.\n",
    "    # The resultant summary will consist of random sentences.\n",
    "    if all(graph.edgeWeight(edge) == 0 for edge in graph.getEdges()):\n",
    "        createGraph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUnreachableNodes(graph):\n",
    "    for node in graph.nodes():\n",
    "        if sum(graph.edgeWeight((node, other)) for other in graph.neighbors(node)) == 0:\n",
    "            graph.delNode(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERGENCE_THRESHOLD = 1e-5\n",
    "ITERATION_TIME = 100\n",
    "\n",
    "# PageRank for underected graph\n",
    "def pagerankWeighted(graph, damping=0.85):\n",
    "    initialValue = 1.0 / len(graph.nodes())\n",
    "    scores = dict.fromkeys(graph.nodes(), initialValue)\n",
    "\n",
    "    for epoch in range(ITERATION_TIME):\n",
    "        convergenceAchieved = 0\n",
    "        for i in graph.nodes():\n",
    "            rank = 1 - damping\n",
    "            for j in graph.neighbors(i):\n",
    "                neighborsSum = sum(graph.edgeWeight((j, k)) for k in graph.neighbors(j))\n",
    "                rank += damping * scores[j] * graph.edgeWeight((j, i)) / neighborsSum\n",
    "\n",
    "            if abs(scores[i] - rank) <= CONVERGENCE_THRESHOLD:\n",
    "                convergenceAchieved += 1\n",
    "\n",
    "            scores[i] = rank\n",
    "\n",
    "        if convergenceAchieved == len(graph.nodes()):\n",
    "            break\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addScoresToSentences(sentences, scores):\n",
    "    for sentence in sentences:\n",
    "        # Adds the score to the object if it has one.\n",
    "        if sentence.token in scores:\n",
    "            sentence.score = scores[sentence.token]\n",
    "        else:\n",
    "            sentence.score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getSentencesWithWordCount(sentences, words):\n",
    "    wordCount = 0\n",
    "    selectedSentences = []\n",
    "    # Loops until the word count is reached.\n",
    "    for sentence in sentences:\n",
    "        wordsInSentence = len(sentence.text.split())\n",
    "\n",
    "        # Checks if the inclusion of the sentence gives a better approximation\n",
    "        # to the word parameter.\n",
    "        if abs(words - wordCount - wordsInSentence) > abs(words - wordCount):\n",
    "            return selectedSentences\n",
    "\n",
    "        selectedSentences.append(sentence)\n",
    "        wordCount += wordsInSentence\n",
    "\n",
    "    return selectedSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.2\n",
    "\n",
    "def extractMostImportantSentences(sentences, maxWords=None):\n",
    "    sentences.sort(key=lambda s: s.score, reverse=True)\n",
    "    # If no \"maxWords\" option is selected, the number of sentences is\n",
    "    # reduced by the provided ratio.\n",
    "    if maxWords is None:\n",
    "        length = len(sentences) * ratio\n",
    "        return sentences[:int(length)]\n",
    "\n",
    "    # Else, the ratio is ignored.\n",
    "    else:\n",
    "        return getSentencesWithWordCount(sentences, maxWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(path: str, maxWords: int):\n",
    "    # Preprocessing\n",
    "    sentences = getCleanedSentences(path)\n",
    "\n",
    "    # Creates the graph and calculates the similarity coefficient for every pair of nodes.\n",
    "    graph = buildGraph([sentence.token for sentence in sentences])\n",
    "    setGraphEdgeWeights(graph)\n",
    "    \n",
    "    # Remove all nodes with all edges weights equal to zero.\n",
    "    removeUnreachableNodes(graph)\n",
    "\n",
    "    # We cannot run PageRank in an empty graph.\n",
    "    if len(graph.nodes()) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    # Ranks the tokens using the PageRank algorithm. Returns dict of sentence -> score\n",
    "    pagerankScores = pagerankWeighted(graph)\n",
    "\n",
    "    # Adds the scores to the sentence objects.\n",
    "    addScoresToSentences(sentences, pagerankScores)\n",
    "\n",
    "    # Extracts the most important sentences with the selected criterion.\n",
    "    extractedSentences = extractMostImportantSentences(sentences)\n",
    "    # Print summary consisting of only 100 words\n",
    "    # print(\"\\n\".join([sentence.text for sentence in extractMostImportantSentences(sentences, maxWords)]))\n",
    "\n",
    "    # Sorts the extracted sentences by apparition order in the original text.\n",
    "    extractedSentences.sort(key=lambda s: s.index)\n",
    "\n",
    "    return \"\\n\".join([sentence.text for sentence in extractedSentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "outputPath = path.replace('.','.summay_')\n",
    "if not os.path.exists(outputPath):\n",
    "    os.makedirs(outputPath)\n",
    "\n",
    "maxWords = 100\n",
    "\n",
    "for article in articles:\n",
    "    with open(join(outputPath, article.replace('.txt', '_summary.txt')),'w', encoding='utf-8') as article_summary:\n",
    "        article_summary.write(summarize(join(path, article), maxWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Gensim\n",
    "PAT_ALPHABETIC = re.compile('(((?![\\d])\\w)+)', re.UNICODE)\n",
    "def tokenize(text, lowercase=False):\n",
    "    \"\"\"\n",
    "    Iteratively yield tokens as unicode strings, optionally also lowercasing them\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    for match in PAT_ALPHABETIC.finditer(text):\n",
    "        yield match.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried using bulgarian-nlp POS Tagger but is has a bug and therefore is not running, so we'll have to do without tagging\n",
    "\n",
    "# from models.taggers import POSTagger\n",
    "\n",
    "# pos_tagger = POSTagger()\n",
    "# pos_tagger.generate_tags('Аз сьм мьж.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(text):\n",
    "    # return pos_tagger.generate_tags(text)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCleanedWords(text):\n",
    "    \"\"\" Tokenizes a given text into words, applying filters and lemmatizing them.\n",
    "    Returns a dict of word -> syntacticUnit. \"\"\"\n",
    "    originalWords = list(tokenize(text, lowercase=True))\n",
    "    filteredWords = filterWords(originalWords)\n",
    "    tags = tag(\" \".join(originalWords))  # tag needs the context of the words in the text\n",
    "\n",
    "    units = mergeSyntacticUnits(originalWords, filteredWords)\n",
    "    return { unit.text : unit for unit in units }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDING_FILTER = ['NOUN', 'ADJECTIVE']\n",
    "EXCLUDING_FILTER = []\n",
    "\n",
    "\n",
    "def getPOSFilters():\n",
    "    return frozenset(INCLUDING_FILTER), frozenset(EXCLUDING_FILTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsForGraph(tokens):\n",
    "    \"\"\" Filter the words by their pos tagging\"\"\"\n",
    "    includeFilters, excludeFilters = getPOSFilters()\n",
    "    if includeFilters and excludeFilters:\n",
    "        raise ValueError(\"Can't use both include and exclude filters, should use only one\")\n",
    "\n",
    "    result = []\n",
    "    for word, unit in tokens.items():\n",
    "        if excludeFilters and unit.tag in excludeFilters:\n",
    "            continue\n",
    "        if (includeFilters and unit.tag in includeFilters) or not includeFilters or not unit.tag:\n",
    "            result.append(unit.token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setGraphEdge(graph, tokens, wordA, wordB):\n",
    "    if wordA in tokens and wordB in tokens:\n",
    "        lemmaA = tokens[wordA].token\n",
    "        lemmaB = tokens[wordB].token\n",
    "        edge = (lemmaA, lemmaB)\n",
    "\n",
    "        if graph.hasNode(lemmaA) and graph.hasNode(lemmaB) and not graph.hasEdge(edge):\n",
    "            graph.addEdge(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2\n",
    "from itertools import combinations \n",
    "\n",
    "def processFirstWindow(graph, tokens, split_text):\n",
    "    first_window = split_text[:WINDOW_SIZE]\n",
    "    for wordA, wordB in combinations(first_window, 2):\n",
    "        setGraphEdge(graph, tokens, wordA, wordB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "def initQueue(split_text):\n",
    "    queue = Queue()\n",
    "    first_window = split_text[:WINDOW_SIZE]\n",
    "    for word in first_window[1:]:\n",
    "        queue.put(word)\n",
    "    return queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queueIterator(queue):\n",
    "    iterations = queue.qsize()\n",
    "    for i in range(iterations):\n",
    "        var = queue.get()\n",
    "        yield var\n",
    "        queue.put(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateQueue(queue, word):\n",
    "    queue.get()\n",
    "    queue.put(word)\n",
    "    assert queue.qsize() == (WINDOW_SIZE - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processWord(graph, tokens, queue, word):\n",
    "    for word_to_compare in queueIterator(queue):\n",
    "        setGraphEdge(graph, tokens, word, word_to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(graph, tokens, split_text):\n",
    "    queue = initQueue(split_text)\n",
    "    for i in range(WINDOW_SIZE, len(split_text)):\n",
    "        word = split_text[i]\n",
    "        processWord(graph, tokens, queue, word)\n",
    "        updateQueue(queue, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setGraphEdges(graph, tokens, split_text):\n",
    "    processFirstWindow(graph, tokens, split_text)\n",
    "    processText(graph, tokens, split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTokens(lemmas, scores, ratio, maxWords=None):\n",
    "    lemmas.sort(key=lambda s: scores[s], reverse=True)\n",
    "\n",
    "    # If no \"maxWords\" option is selected, the number of sentences is\n",
    "    # reduced by the provided ratio, else, the ratio is ignored.\n",
    "    length = len(lemmas) * ratio if maxWords is None else maxWords\n",
    "    return [(scores[lemmas[i]], lemmas[i],) for i in range(int(length))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmasToWords(tokens):\n",
    "    lemmaToWord = {}\n",
    "    for word, unit in tokens.items():\n",
    "        lemma = unit.token\n",
    "        if lemma in lemmaToWord:\n",
    "            lemmaToWord[lemma].append(word)\n",
    "        else:\n",
    "            lemmaToWord[lemma] = [word]\n",
    "    return lemmaToWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeywordsWithScore(extractedLemmas, lemmaToWord):\n",
    "    keywords = {}\n",
    "    for score, lemma in extractedLemmas:\n",
    "        keywordList = lemmaToWord[lemma]\n",
    "        for keyword in keywordList:\n",
    "            keywords[keyword] = score\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stripWord(word):\n",
    "    strippedWordList = list(tokenize(word))\n",
    "    return strippedWordList[0] if strippedWordList else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinedKeywords(keywords, splitText):\n",
    "    result = []\n",
    "    keywords = keywords.copy()\n",
    "    textSize = len(splitText)\n",
    "    for i in range(textSize):\n",
    "        word = stripWord(splitText[i])\n",
    "        if word in keywords:\n",
    "            combinedWord = [word]\n",
    "            if i + 1 == textSize:\n",
    "                result.append(word)   # appends last word if keyword and doesn't iterate\n",
    "            for j in range(i + 1, textSize):\n",
    "                otherWord = stripWord(splitText[j])\n",
    "                if otherWord in keywords and otherWord == splitText[j] \\\n",
    "                        and otherWord not in combinedWord:\n",
    "                    combinedWord.append(otherWord)\n",
    "                else:\n",
    "                    for keyword in combinedWord:\n",
    "                        keywords.pop(keyword)\n",
    "                    result.append(\" \".join(combinedWord))\n",
    "                    break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAverageScore(concept, keywords):\n",
    "    wordList = concept.split()\n",
    "    wordCounter = 0\n",
    "    total = 0\n",
    "    for word in wordList:\n",
    "        total += keywords[word]\n",
    "        wordCounter += 1\n",
    "    return total / wordCounter\n",
    "\n",
    "\n",
    "def formatResults(keywords, combinedkeywords, split=False, scores=False):\n",
    "    combinedkeywords.sort(key=lambda w: getAverageScore(w, keywords), reverse=True)\n",
    "    if scores:\n",
    "        return [(word, getAverageScore(word, keywords)) for word in combinedkeywords]\n",
    "    if split:\n",
    "        return combinedkeywords\n",
    "    return \"\\n\".join(combinedkeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxWords = 5\n",
    "\n",
    "def keywords(path):\n",
    "    with open(path, encoding='utf-8') as article:\n",
    "        text =  ' '.join(article.read().splitlines())\n",
    "\n",
    "    # Gets a dict of word -> lemma\n",
    "    tokens = getCleanedWords(text)\n",
    "    splitText = list(tokenize(text, lowercase=True))\n",
    "\n",
    "    # Creates the graph and adds the edges\n",
    "    graph = buildGraph(getWordsForGraph(tokens))\n",
    "    setGraphEdges(graph, tokens, splitText)\n",
    "    del splitText # It's no longer used\n",
    "\n",
    "    removeUnreachableNodes(graph)\n",
    "\n",
    "    # PageRank cannot be run in an empty graph.\n",
    "    if len(graph.nodes()) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    # Ranks the tokens using the PageRank algorithm. Returns dict of lemma -> score\n",
    "    pagerankScores = pagerankWeighted(graph)\n",
    "\n",
    "    extractedLemmas = extractTokens(graph.nodes(), pagerankScores, ratio)\n",
    "\n",
    "    lemmasToWord = lemmasToWords(tokens)\n",
    "    keywords = getKeywordsWithScore(extractedLemmas, lemmasToWord)\n",
    "\n",
    "    # text.split() to keep numbers and punctuation marks, so separeted concepts are not combined\n",
    "    combinedKeywords = getCombinedKeywords(keywords, text.split())\n",
    "\n",
    "    return formatResults(keywords, combinedKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = path.replace('.','.keywords_')\n",
    "if not os.path.exists(outputPath):\n",
    "    os.makedirs(outputPath)\n",
    "\n",
    "for article in articles:\n",
    "    with open(join(outputPath, article.replace('.txt', '_keywords.txt')),'w', encoding='utf-8') as article_summary:\n",
    "        article_summary.write(keywords(join(path, article)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
